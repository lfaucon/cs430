\documentclass[11pt]{article}

\usepackage{amsmath}
\usepackage{textcomp}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}
\DeclareMathOperator*{\argmax}{argmax}
\usepackage[]{algorithm2e}
\SetArgSty{textnormal}
% add other packages here

% put your group number and names in the author field
\title{\bf Exercise 2: A Reactive Agent for the Pickup and Delivery Problem}
\author{Group \textnumero: Student 1, Student 2}

% the report should not be longer than 3 pages

\begin{document}
\maketitle

\section{Problem Representation}

\subsection{Representation Description}
% describe how you design the state representation, the possible actions, the reward table and the probability transition table

States correspond to the current city of the agent and the available task. We decided to represent them as a pair \verb|(cityFrom, cityTo)| where \verb|cityTo| is the city of delivery of the task and can be \verb|null| if there is no task available. From a state \verb|(cityFrom, cityTo)|, possible actions are either to pickup -- if \verb|cityTo| is not \verb|null| -- the task (going to \verb|cityTo|) or -- in any case -- to move to a neighbor of \verb|cityFrom|. The reward $R(s,a)$ for a pair [state $s$; action $a$] is the value of the delivery (if the action is a pickup otherwise 0) minus the \verb|costPerKm| times the distance traveled.

\subsection{Implementation Details}
% describe the implementation details of the representations above and the implementation details of the reinforcement learning algorithm you implemented
\subsubsection{Representations}
The set of possible states is implemented as a HashMap of HashMaps:
\begin{verbatim}
states = new HashMap<City, HashMap<City, State>>();
\end{verbatim}
To each city \verb|cityFrom| is associated an hash map \verb|hm| that associates each possible task \verb|cityTo| available in \verb|cityFrom| and the \verb|null| task with an object \verb|State|.\\
\\
An object \verb|State| contains 6 parameters:

\begin{itemize}
\item \verb|City cityFrom|: \small{\textit{city where is the car}}\\
\item \verb|City cityTo|: \small{\textit{city where is the car}}\\
\item \verb|HashMap<City,Double> values|: \small{\textit{associating possible actions (neighboring cities + cityTo) with Q-values}}\\
\item \verb|HashMap<City,Double> rewards|: \small{\textit{associating possible actions (neighboring cities + cityTo) with rewards}}\\
\item \verb|double bestValue|: \small{\textit{value $V(s) = \max_a Q(s,a)$}}\\
\item \verb|City bestAction|: \small{\textit{policy $\pi(s) = \argmax_a Q(s,a)$}}\\
\end{itemize}

While setting up the reactive agent's class \verb|ReactiveMDP|, we fill the hash map of states \verb|States| with created object states for each possible pair \verb|(cityFrom, cityTo)|, and we also initialize Q-values and rewards for each state by looping over possible actions (neighboring cities + \verb|cityTo| if not \verb|null|).

\subsubsection{Value Iterations}
Then, the value of each \verb|State| $s$ contained in the hashmap \verb|states| is updated with a value iteration step:

\RestyleAlgo{boxruled}
\begin{algorithm}[h]
    $\text{oldValue} = V(s)$\\
	\For{\textit{action} $a$ in State $s$}{
        $\text{newCityFrom} \leftarrow a$\\
		$Q(s,a)\leftarrow R(s,a)$\\
        \For{$\textit{each city} \text{ newCityTo}$}{
            $s' \leftarrow \text{[newCityFrom ; newCityTo]}$\\
            $Q(s,a) \leftarrow Q(s,a) + \gamma \mathcal{P}\left[\textit{task from }\text{newCityFrom }\textit{to }\text{newCityTo}\right] V(s')$ 
        }
        $Q(s,a) \leftarrow Q(s,a) + \gamma \mathcal{P}\left[\textit{no task from }\text{newCityFrom}\right] V(s')$\\
	}
    $V(s) = \max_a Q(s,a)$\\
    $\pi(s) = \argmax_a Q(s,a)$\\
    $\delta(s) = \left( \text{oldValue} - V(s) \right)^2$
	\caption{\textbf{Value iterations for state $s$}.}
	\label{algo}
\end{algorithm}

When value iterations converge to fixed point $V^*$ (when $\sum_s \delta(s) < 1e^{-6}$), we stop value iterations and we launch the MDP with computed policies $\pi(s) \forall s$.

\section{Results}
% in this section, you describe several results from the experiments with your reactive agent

\subsection{Experiment 1: Discount factor}
% the purpose of this experiment is to understand how the discount factor influences the result

\subsubsection{Setting}
% you describe how you perform the experiment (you also need to specify the configuration used for the experiment)
We ran our optimisation algorithm for discount values of $0.01$, $0.25$, $0.50$, $0.75$, and $0.99$

\textbf{Command line:}
\begin{verbatim}
java -jar ../logist/logist.jar config/reactive.xml reactive-.01 reactive-.25 ... 
... reactive-.50 reactive-.75 reactive-.99
\end{verbatim}

\subsubsection{Observations}
% you describe the experimental results and the conclusions you inferred from these results
After letting the simulation run for a long time ($1.5 10^4$ ticks), we observe that discounts factors closer to 1 have a higher average profit.
\begin{verbatim}
reactive-.01
AVERAGE PROFIT: 39580.19921052631

reactive-.25
AVERAGE PROFIT: 41657.42321755028

reactive-.50 
AVERAGE PROFIT: 42159.52963477821

reactive-.75
AVERAGE PROFIT: 42300.31010737628

reactive-.99
AVERAGE PROFIT: 42788.204704532414
\end{verbatim}

\subsection{Experiment 2: Comparisons with dummy agents}
% you compare the results of your agent with two dummy agents: the random agent that was already given in the starter files and another dummy agent that you define and create. You should report the results from the simulations using the topologies given in the starter files and optionally, additional topologies that you create.

\subsubsection{Setting}
% you describe how you perform the experiment and you describe the dummy agent you created (you also need to specify the configuration used for the experiment)
We compare the reactive agent with discount factors of $0.99$ and $0.01$ to two dummy agents. The first one that always do pickUp actions if available and the second one who takes the pickups only with a probability of $50\%$. Both dummy agent move randomly when not doing pickups.

\textbf{Command line:}
\begin{verbatim}
java -jar ../logist/logist.jar config/reactive2.xml reactive-.01 reactive-.99 ... 
... reactive-random reactive-alwaysPickUp
\end{verbatim}


\subsubsection{Observations}
% elaborate on the observed results
We observe that the random agent performs very badly, but that always picking up is not so bad compared to the reactive with discount factor of $0.01$ we can understand  that as a small discount factor means that the agent will care more about immediate rewards than future value, which means always picking up available tasks.

\begin{verbatim}
reactive-.99
AVERAGE PROFIT: 2607.9454202880934

reactive-.01
AVERAGE PROFIT: 2297.9035733919736

reactive-alwaysPickUp
AVERAGE PROFIT: 2246.7879190175904

reactive-random
AVERAGE PROFIT: 968.0287038588261
\end{verbatim}

\subsection{Experiment 3}
% other experiments you would like to present

\subsubsection{Setting}

\textbf{Command line:}
\begin{verbatim}
java -jar ../logist/logist.jar config/reactive3.xml reactive-.01 reactive-.25 ... 
... reactive-.50 reactive-.75 reactive-.99
\end{verbatim}

\textbf{and:}
\begin{verbatim}
java -jar ../logist/logist.jar config/reactive4.xml reactive-.01 reactive-.25 ... 
... reactive-.50 reactive-.75 reactive-.99
\end{verbatim}

\subsubsection{Observations}
These two experiments tend confirm the results of experiment 1 even though the differences between different discount factors are sometime very small.

\end{document}